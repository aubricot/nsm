{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymskt as mskt\n",
    "from NSM.mesh.interpolate import interpolate_points, interpolate_mesh, interpolate_common\n",
    "from NSM.datasets.sdf_dataset import get_pts_center_and_scale\n",
    "from NSM.mesh import create_mesh\n",
    "from NSM.models import TriplanarDecoder\n",
    "import pyvista as pv\n",
    "import vtk\n",
    "import open3d as o3d\n",
    "\n",
    "from itkwidgets import view\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from scipy.spatial import cKDTree\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# Define model parameters to use\n",
    "TRAIN_DIR = \"run_v30\" # TO DO: Choose training directory containing model ckpt and latent codes\n",
    "cwd = os.getcwd()\n",
    "if TRAIN_DIR not in cwd:\n",
    "    os.chdir(cwd + '/' + TRAIN_DIR)\n",
    "print(\"\\033[92mWorking directory set to: \", os.getcwd())\n",
    "CKPT = '3000' # TO DO: Choose the ckpt value you want to analyze results for\n",
    "LC_PATH = 'latent_codes' + '/' + CKPT + '.pth'\n",
    "MODEL_PATH = 'model' + '/' + CKPT + '.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "\n",
    "class NumpyTransform:\n",
    "    def __init__(self, matrix):\n",
    "        self.matrix = matrix\n",
    "    def GetMatrix(self):\n",
    "        vtk_mat = vtk.vtkMatrix4x4()\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                vtk_mat.SetElement(i, j, self.matrix[i, j])\n",
    "        return vtk_mat\n",
    "\n",
    "def load_config(config_path='model_params_config.json'):\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        print(f\"\\033[92mLoaded config from {config_path}\\033[0m\")\n",
    "        return config\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Error: model_params_config.json not found at {config_path}\")\n",
    "\n",
    "def load_model_and_latents(MODEL_PATH, LC_PATH, config, device):\n",
    "    # Load model\n",
    "    triplane_args = {\n",
    "        'latent_dim': config['latent_size'],\n",
    "        'n_objects': config['objects_per_decoder'],\n",
    "        'conv_hidden_dims': config['conv_hidden_dims'],\n",
    "        'conv_deep_image_size': config['conv_deep_image_size'],\n",
    "        'conv_norm': config['conv_norm'], \n",
    "        'conv_norm_type': config['conv_norm_type'],\n",
    "        'conv_start_with_mlp': config['conv_start_with_mlp'],\n",
    "        'sdf_latent_size': config['sdf_latent_size'],\n",
    "        'sdf_hidden_dims': config['sdf_hidden_dims'],\n",
    "        'sdf_weight_norm': config['weight_norm'],\n",
    "        'sdf_final_activation': config['final_activation'],\n",
    "        'sdf_activation': config['activation'],\n",
    "        'sdf_dropout_prob': config['dropout_prob'],\n",
    "        'sum_sdf_features': config['sum_conv_output_features'],\n",
    "        'conv_pred_sdf': config['conv_pred_sdf'],\n",
    "    }\n",
    "    model = TriplanarDecoder(**triplane_args)\n",
    "    model_ckpt = torch.load(MODEL_PATH, map_location=device)\n",
    "    model.load_state_dict(model_ckpt['model'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    # Load latents\n",
    "    latent_ckpt = torch.load(LC_PATH, map_location=device)\n",
    "    latent_codes = latent_ckpt['latent_codes']['weight'].detach().cpu()\n",
    "    return model, latent_ckpt, latent_codes\n",
    "\n",
    "class MeshWrapper:\n",
    "    def __init__(self, mesh):\n",
    "        self.mesh = mesh\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.mesh, name)\n",
    "    @property\n",
    "    def scalar_names(self):\n",
    "        return []\n",
    "    @property\n",
    "    def point_coords(self):\n",
    "        return self.mesh.points\n",
    "    @point_coords.setter\n",
    "    def point_coords(self, value):\n",
    "        self.mesh.points = value\n",
    "\n",
    "def normalize_latent(v):\n",
    "    v = np.array(v, dtype=np.float64).reshape(-1)\n",
    "    norm = np.linalg.norm(v)\n",
    "    if not np.isfinite(norm) or norm < 1e-9:\n",
    "        raise ValueError(f\"Invalid latent vector: norm={norm}\")\n",
    "    return v / norm\n",
    "\n",
    "def load_mrk_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    if \"markups\" not in data or len(data[\"markups\"]) == 0:\n",
    "        raise ValueError(f\"No 'markups' found in {path}\")\n",
    "    markups = data[\"markups\"][0]\n",
    "    points = []\n",
    "    labels = []\n",
    "    for cp in markups.get(\"controlPoints\", []):\n",
    "        pos = cp.get(\"position\", None)\n",
    "        if pos is not None:\n",
    "            points.append(pos)\n",
    "            labels.append(cp.get(\"label\", None))\n",
    "    points = np.array(points, dtype=np.float32)\n",
    "    return points, labels\n",
    "\n",
    "def vtk_to_o3d(vtk_file):\n",
    "    # Load the VTK file using PyVista\n",
    "    vtk_mesh = pv.read(vtk_file)\n",
    "    # Extract vertices and faces from the VTK mesh\n",
    "    vertices = vtk_mesh.points\n",
    "    faces = vtk_mesh.faces.reshape((-1, 4))[:, 1:]  # Reshape faces to get rid of the first number\n",
    "    # Create Open3D TriangleMesh from the extracted vertices and faces\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(vertices)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    # Optionally, check if normals exist in the VTK file\n",
    "    if vtk_mesh.point_data.get(\"Normals\") is not None:\n",
    "        mesh.vertex_normals = o3d.utility.Vector3dVector(vtk_mesh.point_data[\"Normals\"])\n",
    "    return mesh\n",
    "\n",
    "def project_landmarks_to_surface(landmarks, target_mesh):\n",
    "    mesh_points = np.asarray(target_mesh.points)\n",
    "    kdtree = cKDTree(mesh_points)\n",
    "    _, idx = kdtree.query(landmarks)\n",
    "    projected = mesh_points[idx]\n",
    "    return projected\n",
    "\n",
    "def visualize_landmark_interpolation_external(original_mesh, mesh, original_pts, interp_pts, labels=None):\n",
    "    # Unwrap MeshWrapper if needed\n",
    "    if hasattr(mesh, \"mesh\"):\n",
    "        mesh = mesh.mesh\n",
    "    # Ensure both meshes are PyVista PolyData\n",
    "    mesh_pv = mesh if isinstance(mesh, pv.PolyData) else mesh.extract_geometry()\n",
    "    original_mesh_pv = original_mesh if isinstance(original_mesh, pv.PolyData) else original_mesh.extract_geometry()\n",
    "    # Compute normals for smoother rendering\n",
    "    mesh_pv = mesh_pv.compute_normals(cell_normals=False, point_normals=True, inplace=False)\n",
    "    original_mesh_pv = original_mesh_pv.compute_normals(cell_normals=False, point_normals=True, inplace=False)\n",
    "    # Landmarks as PolyData\n",
    "    orig = pv.PolyData(original_pts)\n",
    "    interp = pv.PolyData(interp_pts)\n",
    "    # Lines connecting original â†’ interpolated\n",
    "    lines = [pv.Line(original_pts[i], interp_pts[i]) for i in range(len(original_pts))]\n",
    "    lines_polydata = lines[0]\n",
    "    for line in lines[1:]:\n",
    "        lines_polydata = lines_polydata.merge(line)\n",
    "    # ---- Interactive window ----\n",
    "    plotter = pv.Plotter(window_size=[1024, 768], notebook=False, off_screen=False)\n",
    "    plotter.add_text(\"Landmark Interpolation\", font_size=12)\n",
    "    # Add meshes and landmarks\n",
    "    plotter.add_mesh(original_mesh_pv, color=\"lightgray\", opacity=0.4, smooth_shading=True, show_edges=False)\n",
    "    plotter.add_mesh(mesh_pv, color=\"blue\", opacity=0.4, smooth_shading=True, show_edges=False)\n",
    "    plotter.add_mesh(orig, color=\"red\", point_size=14, render_points_as_spheres=True)\n",
    "    plotter.add_mesh(interp, color=\"green\", point_size=14, render_points_as_spheres=True)\n",
    "    plotter.add_mesh(lines_polydata, color=\"orange\", line_width=2)\n",
    "    # Optional labels\n",
    "    if labels is not None:\n",
    "        for pt, label in zip(original_pts, labels):\n",
    "            plotter.add_point_labels([pt], [label], font_size=10, text_color=\"black\")\n",
    "    # Show in a standalone GUI window\n",
    "    plotter.show_axes()\n",
    "    plotter.show_grid()\n",
    "    plotter.show(interactive=True, auto_close=True)\n",
    "\n",
    "def visualize_landmarks_on_mesh(original_mesh, original_pts):\n",
    "    # Ensure both meshes are PyVista PolyData\n",
    "    original_mesh_pv = original_mesh if isinstance(original_mesh, pv.PolyData) else original_mesh.extract_geometry()\n",
    "    # Compute normals for smoother rendering\n",
    "    original_mesh_pv = original_mesh_pv.compute_normals(cell_normals=False, point_normals=True, inplace=False)\n",
    "    # Landmarks as PolyData\n",
    "    orig = pv.PolyData(original_pts)\n",
    "    # ---- Interactive window ----\n",
    "    plotter = pv.Plotter(window_size=[1024, 768], notebook=False, off_screen=False)\n",
    "    plotter.add_text(\"Landmark Interpolation\", font_size=12)\n",
    "    # Add meshes and landmarks\n",
    "    plotter.add_mesh(original_mesh_pv, color=\"lightgray\", opacity=0.4, smooth_shading=True, show_edges=False)\n",
    "    plotter.add_mesh(orig, color=\"red\", point_size=14, render_points_as_spheres=True)\n",
    "    plotter.show_axes()\n",
    "    plotter.show_grid()\n",
    "    # Show in a standalone GUI window\n",
    "    plotter.show(interactive=True, auto_close=True)\n",
    "\n",
    "def normalize_to_unit_sphere(landmarks, reference_points=None):\n",
    "    if reference_points is None:\n",
    "        reference_points = landmarks\n",
    "    center = np.mean(reference_points, axis=0)\n",
    "    shifted = landmarks - center\n",
    "    scale = np.max(np.linalg.norm(reference_points - center, axis=1))\n",
    "    normalized = shifted / scale\n",
    "    return normalized, center, scale\n",
    "\n",
    "def normalize_mesh_to_unit_sphere(mesh, reference_points=None):\n",
    "    # Extract vertices\n",
    "    if isinstance(mesh, pv.PolyData):\n",
    "        vertices = mesh.points.copy()\n",
    "    else:  # assume numpy array of shape (N, 3)\n",
    "        vertices = mesh.copy()\n",
    "    if reference_points is None:\n",
    "        reference_points = vertices\n",
    "    # Compute center and scale\n",
    "    center = np.mean(reference_points, axis=0)\n",
    "    shifted = vertices - center\n",
    "    scale = np.max(np.linalg.norm(reference_points - center, axis=1))\n",
    "    normalized_vertices = shifted / scale\n",
    "    # If PyVista mesh, return new mesh\n",
    "    if isinstance(mesh, pv.PolyData):\n",
    "        normalized_mesh = mesh.copy()\n",
    "        normalized_mesh.points = normalized_vertices\n",
    "        return normalized_mesh, center, scale\n",
    "    else:\n",
    "        return normalized_vertices, center, scale\n",
    "    \n",
    "def o3d_to_pv(mesh_o3d):\n",
    "    # Get vertices as numpy array\n",
    "    vertices = np.asarray(mesh_o3d.vertices)\n",
    "    # Get faces (Open3D uses shape (n_faces, 3))\n",
    "    faces = np.asarray(mesh_o3d.triangles)\n",
    "    # PyVista expects faces in a flattened format: [3, v0, v1, v2, 3, ...]\n",
    "    faces_flat = np.hstack([np.full((faces.shape[0], 1), 3), faces]).flatten()\n",
    "    \n",
    "    # Create PyVista mesh\n",
    "    mesh_pv = pv.PolyData(vertices, faces_flat)\n",
    "    return mesh_pv\n",
    "\n",
    "def save_mrk_json(points, labels, output_filename):\n",
    "    control_points = []\n",
    "    for i, (pt, label) in enumerate(zip(points, labels), start=1):\n",
    "        # Ensure pt is a list or numpy array of coordinates\n",
    "        if isinstance(pt, np.ndarray):\n",
    "            pt = pt.tolist()  # If pt is a numpy array, convert to a list\n",
    "        elif isinstance(pt, list):\n",
    "            pass  # pt is already a list, no conversion needed\n",
    "        else:\n",
    "            raise ValueError(f\"Expected pt to be a list or numpy array, got {type(pt)}\")\n",
    "        \n",
    "        cp = {\n",
    "            \"id\": str(i),\n",
    "            \"label\": label,\n",
    "            \"description\": \"\",\n",
    "            \"associatedNodeID\": \"\",\n",
    "            \"position\": pt,  # No need to call .tolist() here if it's already a list\n",
    "            \"orientation\": [-1.0, -0.0, -0.0, -0.0, -1.0, -0.0, 0.0, 0.0, 1.0],\n",
    "            \"selected\": True,\n",
    "            \"locked\": True,  # Lock the position so it can't be moved in Slicer\n",
    "            \"lockedPosition\": True,  # Optional: If you want explicit \"lockedPosition\" field\n",
    "            \"visibility\": True,\n",
    "            \"positionStatus\": \"defined\"  # Ensure the position is defined and not undefined\n",
    "        }\n",
    "        control_points.append(cp)\n",
    "\n",
    "    markups_data = {\n",
    "        \"@schema\": \"https://raw.githubusercontent.com/slicer/slicer/master/Modules/Loadable/Markups/Resources/Schema/markups-schema-v1.0.3.json#\",\n",
    "        \"markups\": [\n",
    "            {\n",
    "                \"type\": \"Fiducial\",\n",
    "                \"coordinateSystem\": \"LPS\",\n",
    "                \"coordinateUnits\": \"mm\",\n",
    "                \"locked\": False,  # Keep the overall markup locked status as False for editing purposes\n",
    "                \"fixedNumberOfControlPoints\": False,\n",
    "                \"labelFormat\": \"%N-%d\",\n",
    "                \"lastUsedControlPointNumber\": len(control_points),\n",
    "                \"controlPoints\": control_points,\n",
    "                \"measurements\": [],\n",
    "                \"display\": {\n",
    "                    \"visibility\": False,\n",
    "                    \"opacity\": 1.0,\n",
    "                    \"color\": [0.4, 1.0, 1.0],\n",
    "                    \"selectedColor\": [1.0, 0.5000076295109483, 0.5000076295109483],\n",
    "                    \"activeColor\": [0.4, 1.0, 0.0],\n",
    "                    \"propertiesLabelVisibility\": False,\n",
    "                    \"pointLabelsVisibility\": False,\n",
    "                    \"textScale\": 3.0,\n",
    "                    \"glyphType\": \"Sphere3D\",\n",
    "                    \"glyphScale\": 3.0,\n",
    "                    \"glyphSize\": 5.0,\n",
    "                    \"useGlyphScale\": True,\n",
    "                    \"sliceProjection\": False,\n",
    "                    \"sliceProjectionUseFiducialColor\": True,\n",
    "                    \"sliceProjectionOutlinedBehindSlicePlane\": False,\n",
    "                    \"sliceProjectionColor\": [1.0, 1.0, 1.0],\n",
    "                    \"sliceProjectionOpacity\": 0.6,\n",
    "                    \"lineThickness\": 0.2,\n",
    "                    \"lineColorFadingStart\": 1.0,\n",
    "                    \"lineColorFadingEnd\": 10.0,\n",
    "                    \"lineColorFadingSaturation\": 1.0,\n",
    "                    \"lineColorFadingHueOffset\": 0.0,\n",
    "                    \"handlesInteractive\": False,\n",
    "                    \"translationHandleVisibility\": True,\n",
    "                    \"rotationHandleVisibility\": True,\n",
    "                    \"scaleHandleVisibility\": False,\n",
    "                    \"interactionHandleScale\": 3.0,\n",
    "                    \"snapMode\": \"toVisibleSurface\"}}]}\n",
    "\n",
    "    # Save to JSON file\n",
    "    with open(output_filename, 'w') as f:\n",
    "        json.dump(markups_data, f, indent=4)\n",
    "    print(f\"Saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "config = load_config()\n",
    "device = config.get(\"device\", \"cuda:0\")\n",
    "train_paths = config['list_mesh_paths']\n",
    "all_vtk_files = [os.path.basename(f) for f in train_paths]\n",
    "\n",
    "# Load model and latent codes\n",
    "model, latent_ckpt, latent_codes = load_model_and_latents(MODEL_PATH, LC_PATH, config, device)\n",
    "\n",
    "# Mesh creation params\n",
    "recon_grid_origin = 1.0\n",
    "n_pts_per_axis = 128\n",
    "voxel_origin = (-recon_grid_origin, -recon_grid_origin, -recon_grid_origin)\n",
    "voxel_size = (recon_grid_origin * 2) / (n_pts_per_axis - 1)\n",
    "offset = np.array([0.0, 0.0, 0.0])\n",
    "scale = 1.0\n",
    "icp_transform = NumpyTransform(np.eye(4))\n",
    "objects = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the specimen closest to the median, and use it and it's landmarks as a template\n",
    "\n",
    "# Move latent codes to device\n",
    "latent_codes = latent_codes.to(device)\n",
    "\n",
    "# Compute median latent\n",
    "median_latent = torch.median(latent_codes, dim=0).values  # shape: (latent_dim,)\n",
    "\n",
    "# Normalize both sets of vectors to unit length\n",
    "latent_norm = F.normalize(latent_codes, p=2, dim=1)\n",
    "median_latent_norm = F.normalize(median_latent.unsqueeze(0), p=2, dim=1)\n",
    "\n",
    "# Cosine similarity and distance\n",
    "cosine_similarities = torch.mm(latent_norm, median_latent_norm.T).squeeze()\n",
    "cosine_distances = 1 - cosine_similarities\n",
    "\n",
    "# Closest latent by cosine\n",
    "med_idx = torch.argmin(cosine_distances).item()\n",
    "\n",
    "print(f\"Closest latent to median (cosine): {all_vtk_files[med_idx]}\")\n",
    "\n",
    "med_fn = \"../vertebrae_meshes/\" + all_vtk_files[med_idx]\n",
    "med_lm_fn = '../alignedLMs/' + os.path.splitext(all_vtk_files[med_idx])[0] + '.mrk.json'\n",
    "print(\"Associated landmark file: \", med_lm_fn, \"\\033[0m\\n\")\n",
    "\n",
    "med_mesh = vtk_to_o3d(med_fn)\n",
    "med_mesh = o3d_to_pv(med_mesh)\n",
    "med_lms, med_labels = load_mrk_json(med_lm_fn)\n",
    "median_mesh, mesh_center, mesh_scale = normalize_mesh_to_unit_sphere(med_mesh)\n",
    "normalized_lms = (med_lms - mesh_center) / mesh_scale\n",
    "median_lms = project_landmarks_to_surface(normalized_lms, median_mesh.mesh if hasattr(median_mesh, \"mesh\") else median_mesh)\n",
    "visualize_landmarks_on_mesh(median_mesh, median_lms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate points from median specimen to random specimens\n",
    "\n",
    "# Number of meshes to sample\n",
    "NUM = 5\n",
    "# create random indices\n",
    "random_indices = np.random.randint(0, len(latent_codes), NUM)\n",
    "\n",
    "# ---- Main interpolation ----\n",
    "interpolated_points = []\n",
    "meshes = []\n",
    "\n",
    "# Loop through randomly selected latent codes for interpolation\n",
    "for i, idx in enumerate(random_indices):\n",
    "    print(f'\\n\\033[92m{i}: {all_vtk_files[idx]}\\033[0m')\n",
    "    # Set out filename for landmarks\n",
    "    out_lm_fn = '../interpedLMs/' + 'interped_' + os.path.splitext(all_vtk_files[idx])[0] + '.mrk.json'\n",
    "    out_mesh_fn = '../interpedMeshes/' + 'interped_' + os.path.splitext(all_vtk_files[idx])[0] + '.vtk'\n",
    "    # Load latent codes by index\n",
    "    latent = latent_codes[idx]\n",
    "    # Create mesh from latent\n",
    "    mesh_out = create_mesh(\n",
    "            decoder=model, latent_vector=latent, n_pts_per_axis=n_pts_per_axis,\n",
    "            voxel_origin=voxel_origin, voxel_size=voxel_size, path_original_mesh=None,\n",
    "            offset=offset, scale=scale, icp_transform=icp_transform,\n",
    "            objects=objects, verbose=False, device=device\n",
    "        )\n",
    "    mesh_out = mesh_out[0] if isinstance(mesh_out, list) else mesh_out\n",
    "    mesh_out.resample_surface(clusters=20_000)\n",
    "\n",
    "    # Normalize latent vectors\n",
    "    latent1 = normalize_latent(median_latent.cpu().numpy())\n",
    "    latent2 = normalize_latent(latent.cpu().numpy())\n",
    "\n",
    "    # --- Interpolate landmarks (these are now in normalized space) ---\n",
    "    interp_pts = interpolate_points(\n",
    "        model=model,\n",
    "        latent1=latent1,\n",
    "        latent2=latent2,\n",
    "        points1=median_lms,\n",
    "        n_steps=100,\n",
    "    )\n",
    "    \n",
    "    normalized_mesh, mesh_center, mesh_scale = normalize_mesh_to_unit_sphere(mesh_out)\n",
    "    normalized_lms = (interp_pts - mesh_center) / mesh_scale\n",
    "    projected_lms = project_landmarks_to_surface(interp_pts, normalized_mesh.mesh if hasattr(normalized_mesh, \"mesh\") else normalized_mesh)\n",
    "    \n",
    "    meshes.append(normalized_mesh)\n",
    "    interpolated_points.append(projected_lms)\n",
    "\n",
    "    # Optional: Save interpolated landmarks and generated meshes\n",
    "    save_mrk_json(projected_lms, med_labels, out_lm_fn)\n",
    "    orig_lm_fn = os.path.splitext(all_vtk_files[idx])[0] + '.mrk.json'\n",
    "    shutil.copy('../alignedLMs/' + orig_lm_fn, '../interpedLMs/' + orig_lm_fn)\n",
    "    print(f\"Copied original landmark file to {'../interpedLMs/' + orig_lm_fn} for comparison with interpolated landmarks.\")\n",
    "    normalized_mesh.save(out_mesh_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize interpolated landmarks and generated mesh in interactive viewer\n",
    "\n",
    "# Pick index to inspect\n",
    "idx_to_show = 2 # change index as needed\n",
    "pv.set_jupyter_backend(None)  # disable notebook mode completely\n",
    "\n",
    "visualize_landmark_interpolation_external(\n",
    "    median_mesh, # original mesh\n",
    "    meshes[idx_to_show], # generated mesh\n",
    "    median_lms,  # original landmarks\n",
    "    interpolated_points[idx_to_show],  # interpolated landmarks\n",
    "    med_labels,  # labels\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NSM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
